{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from copy import deepcopy\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from tqdm.notebook import tqdm ## to add progress bars to loops and iterations\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset tweet_eval (C:/Users/Splute/.cache/huggingface/datasets/tweet_eval/sentiment/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba63fdf1dbbf4d74911254222e5bbf48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 45615\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 12284\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download dataset from hungging face\n",
    "dataset = load_dataset(\"tweet_eval\", \"sentiment\")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1077</th>\n",
       "      <td>Wait till this guy finds out Trump cut his Med...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9061</th>\n",
       "      <td>@user Have a listen, these are the feelings of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8577</th>\n",
       "      <td>The best #BlackFriday purchase! #TheWalkingDea...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11261</th>\n",
       "      <td>#Westworld is the greatest work of art I've ev...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8938</th>\n",
       "      <td>Barak Obama | highlighting the many broken pro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "1077   Wait till this guy finds out Trump cut his Med...      0\n",
       "9061   @user Have a listen, these are the feelings of...      0\n",
       "8577   The best #BlackFriday purchase! #TheWalkingDea...      2\n",
       "11261  #Westworld is the greatest work of art I've ev...      2\n",
       "8938   Barak Obama | highlighting the many broken pro...      0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test = pd.DataFrame(dataset['test'])\n",
    "\n",
    "# Balance the amount of each class\n",
    "SEED = 66\n",
    "num_test_per_class = 2000\n",
    "balanced_test_data = data_test.groupby('label', group_keys=False).apply(lambda x:\\\n",
    "        x.sample(min(len(x), num_test_per_class), random_state=SEED)).sample(frac=1, random_state=SEED)\n",
    "# X_test = balanced_test_data.text.tolist()\n",
    "\n",
    "balanced_test_data.head()\n",
    "# X_test.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 2, 2, 0])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = balanced_test_data\n",
    "y_test = df['label'].to_numpy().astype(int)\n",
    "y_test[:5]\n",
    "# # 添加一个新的整数索引，并将其保存为'id'列\n",
    "# df['id'] = df.reset_index(drop=True).index\n",
    "# # 将'id'列移动到最左边\n",
    "# id_column = df['id']  # 获取'id'列\n",
    "# df.drop(columns=['id'], inplace=True)  # 删除'id'列\n",
    "# df.insert(0, 'id', id_column)  # 将'id'列插入到第一列\n",
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test of VADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.0, 'neu': 0.429, 'pos': 0.571, 'compound': 0.6115}\n"
     ]
    }
   ],
   "source": [
    "test = sia.polarity_scores('I am very happy')\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eef21554ac2f478a9d9fc858b798d0d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.43      0.53      2000\n",
      "           1       0.46      0.63      0.53      2000\n",
      "           2       0.62      0.64      0.63      2000\n",
      "\n",
      "    accuracy                           0.57      6000\n",
      "   macro avg       0.60      0.57      0.57      6000\n",
      "weighted avg       0.60      0.57      0.57      6000\n",
      "\n",
      "Time: 0.949s\n"
     ]
    }
   ],
   "source": [
    "vader_res = []\n",
    "start_time = time.time()\n",
    "for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    text = row['text']\n",
    "    res = sia.polarity_scores(text)\n",
    "    compound_score = res['compound']\n",
    "    # 根据'compound'得分返回整数标签\n",
    "    if compound_score >= 0.35:\n",
    "        label = 2  # 正面情感\n",
    "    elif compound_score <= -0.35:\n",
    "        label = 0  # 负面情感\n",
    "    else:\n",
    "        label = 1  # 中性情感\n",
    "    vader_res.append(label)\n",
    "    # break\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(classification_report(y_test, vader_res))\n",
    "print(\"Time: {:.3f}s\".format(elapsed_time))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text of roBerta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = f\"cardiffnlp/twitter-xlm-roberta-base-sentiment\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = 'How would you feel, if I... if I gave you your copy in person?'\n",
    "encoded_text = tokenizer(example, return_tensors='pt')\n",
    "output = model(**encoded_text)\n",
    "scores = output[0][0].detach().numpy()\n",
    "scores = softmax(scores)\n",
    "scores.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polarity_scores_roberta(example):\n",
    "    encoded_text = tokenizer(example, return_tensors='pt')\n",
    "    output = model(**encoded_text)\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "    label = scores.argmax()\n",
    "\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b1c6fea3b854740ba8aef143e92ff37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.87      0.77      2000\n",
      "           1       0.62      0.55      0.58      2000\n",
      "           2       0.84      0.70      0.77      2000\n",
      "\n",
      "    accuracy                           0.71      6000\n",
      "   macro avg       0.71      0.71      0.70      6000\n",
      "weighted avg       0.71      0.71      0.70      6000\n",
      "\n",
      "Time: 447.461s\n"
     ]
    }
   ],
   "source": [
    "# run on the entire dataset\n",
    "roBerta_res = []\n",
    "start_time = time.time()\n",
    "for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    text = row['text']\n",
    "    res = polarity_scores_roberta(text)\n",
    "    roBerta_res.append(res)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(classification_report(y_test, roBerta_res))\n",
    "print(\"Time: {:.3f}s\".format(elapsed_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test of BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import word vectors\n",
    "\n",
    "words = dict()\n",
    "wird_filepath = 'embeddings\\glove.twitter.27B\\glove.twitter.27B.100d.txt'\n",
    "\n",
    "def add_to_dict(d, filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            line = line.split(' ')\n",
    "\n",
    "            try:\n",
    "                d[line[0]] = np.array(line[1:],dtype=float)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "add_to_dict(words, wird_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1193514"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Splute\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use regular expressions to define word segmentation rules,\n",
    "# and split text according to patterns that match regular expressions\n",
    "tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "\n",
    "# find the root-word\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lemmatizer.lemmatize('feet')\n",
    "\n",
    "# preprocess\n",
    "def msg_to_token_list(string):\n",
    "    tokens = tokenizer.tokenize(string)\n",
    "    lowercased_tokens = [token.lower() for token in tokens]\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in lowercased_tokens]\n",
    "    useful_tokens = [token for token in lemmatized_tokens if token in words]\n",
    "\n",
    "    return useful_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def msg_to_word_vectors(msg, word_dict = words):\n",
    "    processed_list_tokens = msg_to_token_list(msg)\n",
    "\n",
    "    vectors = []\n",
    "\n",
    "    for token in processed_list_tokens:\n",
    "        if token not in word_dict:\n",
    "            continue\n",
    "\n",
    "        token_vector = word_dict[token]\n",
    "        vectors.append(token_vector)\n",
    "\n",
    "    return np.array(vectors, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 100)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg_to_word_vectors('@#Did you feel happy?').shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get x and y\n",
    "def dff_to_X_y(dff):\n",
    "    y = dff['label'].to_numpy().astype(int)\n",
    "\n",
    "    all_word_vector_sequences = []\n",
    "\n",
    "    for msg in dff['text']:\n",
    "        msg_as_vector_seq = msg_to_word_vectors(msg)\n",
    "\n",
    "        if msg_as_vector_seq.shape[0] == 0:\n",
    "            msg_as_vector_seq = np.zeros(shape=(1,100))\n",
    "\n",
    "        all_word_vector_sequences.append(msg_as_vector_seq)\n",
    "\n",
    "    return all_word_vector_sequences, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_X(X, desired_sequence_length = 38):\n",
    "    X_copy = deepcopy(X) # create a totally new copy\n",
    "\n",
    "    for i, x in enumerate(X):\n",
    "        x_seq_len = x.shape[0]\n",
    "        sequence_length_difference = desired_sequence_length - x_seq_len\n",
    "\n",
    "        pad = np.zeros(shape=(sequence_length_difference, 100))\n",
    "\n",
    "        X_copy[i] = np.concatenate([x,pad])\n",
    "\n",
    "    return np.array(X_copy).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6000, 38, 100), (6000,))"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test, y_test = dff_to_X_y(df)\n",
    "X_test = pad_X(X_test)\n",
    "\n",
    "X_test.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.63      0.67      2000\n",
      "           1       0.51      0.65      0.57      2000\n",
      "           2       0.75      0.65      0.70      2000\n",
      "\n",
      "    accuracy                           0.64      6000\n",
      "   macro avg       0.66      0.64      0.64      6000\n",
      "weighted avg       0.66      0.64      0.64      6000\n",
      "\n",
      "Time: 2.052s\n"
     ]
    }
   ],
   "source": [
    "best_model = load_model('models/GloVe_LSTM')\n",
    "\n",
    "start_time = time.time()\n",
    "predictions = best_model.predict(X_test)\n",
    "biLSTM_res = []\n",
    "for pred in predictions:\n",
    "  biLSTM_res.append(pred.argmax())\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(classification_report(y_test, biLSTM_res))\n",
    "print(\"Time: {:.3f}s\".format(elapsed_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
